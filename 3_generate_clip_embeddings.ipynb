{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4294c261",
   "metadata": {},
   "source": [
    "Step 3: Extract Key Frames and Generate CLIP Embeddings\n",
    "    1. Here we process each scene-level video clip, extracts the middle frame as a key visual summary, passes it through the CLIP image encoder, and saves the resulting embedding as a .npy file.\n",
    "    2. These embeddings will be used later for semantic search using a text query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c941ca",
   "metadata": {},
   "source": [
    "Setup and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed3578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "SCENE_DIR = 'video_data/scenes'         # Input: Folder containing short video clips (scenes)\n",
    "EMBEDDING_DIR = 'embeddings'            # Output: Folder to save scene embeddings\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n",
    "\n",
    "# Ensure required folders exist\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)\n",
    "\n",
    "# Load CLIP model and processor from Hugging Face\n",
    "# CLIP helps map images and texts into the same embedding space\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8c06e",
   "metadata": {},
   "source": [
    "Function: Extract the middle frame of a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da99031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_middle_frame(video_path):\n",
    "    \"\"\"\n",
    "    Extracts the middle frame from a video file.\n",
    "    Returns the frame as a NumPy array or None if the video can't be read.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open {video_path}\")\n",
    "        return None\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    middle_frame_index = total_frames // 2\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_index)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Failed to read middle frame from {video_path}\")\n",
    "        return None\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab545c38",
   "metadata": {},
   "source": [
    "Function: Generate and return CLIP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231f2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clip_embedding(video_path, model, processor):\n",
    "    \"\"\"\n",
    "    Extracts the middle frame of a video clip and returns a CLIP image embedding.\n",
    "    \"\"\"\n",
    "    frame = extract_middle_frame(video_path)\n",
    "    if frame is None:\n",
    "        return None\n",
    "\n",
    "    # Convert BGR (OpenCV format) to RGB (PIL format)\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Prepare image for CLIP model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Get embedding from CLIP\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        embedding = embedding.cpu().numpy().flatten()  # Flatten to 1D array\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361d9cb",
   "metadata": {},
   "source": [
    "For each scene clip, extract the embedding and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c661cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Generating CLIP embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "scene_files = sorted([\n",
    "    f for f in os.listdir(SCENE_DIR)\n",
    "    if f.lower().endswith('.mp4')\n",
    "])\n",
    "\n",
    "for scene_file in tqdm(scene_files, desc=\"ðŸ”Ž Generating CLIP embeddings\"):\n",
    "    scene_path = os.path.join(SCENE_DIR, scene_file)\n",
    "    embedding = generate_clip_embedding(scene_path, model, processor)\n",
    "\n",
    "    if embedding is not None:\n",
    "        # Save the embedding as a .npy file with the same base name\n",
    "        out_path = os.path.join(EMBEDDING_DIR, scene_file.replace('.mp4', '.npy'))\n",
    "        np.save(out_path, embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
